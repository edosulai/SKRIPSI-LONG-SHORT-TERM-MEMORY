{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "def set_config(config_dict):\n",
    "    config = Config()\n",
    "    config.__dict__ = config_dict\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = set_config({\n",
    "    'row_start': '2004-10-16',\n",
    "    'row_end': '2004-12-14',\n",
    "    \"timestep\": 2,\n",
    "    \"max_batch_size\": 1,\n",
    "    \"layer_size\": 1,\n",
    "    \"unit_size\": 1,\n",
    "    \"dropout\": 0,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"max_epoch\": 50,\n",
    "    \"time_col\": 'tanggal',\n",
    "    \"num_predict\": 5,\n",
    "    \"feature\": ['rr'],\n",
    "    # \"feature\": ['tn','tx','tavg','rh_avg','rr','ss','ff_x','ddd_x','ff_avg','ddd_car'],\n",
    "    \"prediction\": 'rr'\n",
    "})\n",
    "\n",
    "\n",
    "def train_test_split(dataset, timestep=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - timestep):\n",
    "        dataX.append(dataset[i:(i + timestep)])\n",
    "        dataY.append(dataset[i + timestep:i+timestep+1])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "def proyeksi_split(dataset, timestep=1):\n",
    "    dataX = []\n",
    "    for i in range(len(dataset) - timestep + 1):\n",
    "        dataX.append(dataset[i:(i + timestep)])\n",
    "    return np.array(dataX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = pd.read_csv('../Data/1985-2021.csv').replace(to_replace=[8888, 9999, 2555], value=np.nan)\n",
    "DATASETS.interpolate(inplace=True)\n",
    "\n",
    "for feature in DATASETS:\n",
    "  if DATASETS[feature].dtypes == object and feature != config.time_col:\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=1)\n",
    "    tokenizer.fit_on_texts(DATASETS[feature].astype(str))\n",
    "    index_word = list(tokenizer.index_word.values())\n",
    "    word_index = list(tokenizer.word_index.values())\n",
    "    \n",
    "    for index in index_word:\n",
    "      DATASETS = DATASETS.replace(to_replace=str(index.upper()), value=tokenizer.word_index[index], regex=True)\n",
    "      \n",
    "DATASETS = DATASETS.loc[\n",
    "  (DATASETS[config.time_col] >= config.row_start) & (DATASETS[config.time_col] <= config.row_end)\n",
    "]\n",
    "\n",
    "DATELIST = np.array([datetime.strptime(date, '%Y-%m-%d').date() for date in list(DATASETS[config.time_col])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = DATASETS[config.feature]\n",
    "\n",
    "scaller = MinMaxScaler()\n",
    "featuresets_scaled = scaller.fit_transform(featuresets)\n",
    "\n",
    "train_size = int(featuresets_scaled.shape[0] * 0.9)\n",
    "trainset, testset = featuresets_scaled[0:train_size], featuresets_scaled[train_size:featuresets_scaled.shape[0]]\n",
    "traindateset, testdateset = DATELIST[0:train_size], DATELIST[train_size:DATELIST.size]\n",
    "\n",
    "X_train, y_train = train_test_split(trainset, timestep=config.timestep)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], len(config.feature), X_train.shape[1]))\n",
    "y_train = np.reshape(y_train, (y_train.shape[0], len(config.feature), y_train.shape[1]))\n",
    "\n",
    "X_test, y_test = train_test_split(testset, timestep=config.timestep)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], len(config.feature), X_test.shape[1]))\n",
    "y_test = np.reshape(y_test, (y_test.shape[0], len(config.feature), y_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (1, 1, 1)                 16        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16\n",
      "Trainable params: 16\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# y_pred_loss = list()\n",
    "# y_true_loss = list()\n",
    "# loss = list()\n",
    "\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    # y_pred_loss.append(y_pred)\n",
    "    # y_true_loss.append(y_true)\n",
    "    # loss.append(tf.keras.backend.mean(tf.keras.backend.square(y_pred - y_true)))\n",
    "    return tf.keras.backend.mean(tf.keras.backend.square(y_pred - y_true))\n",
    "\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return tf.keras.backend.sqrt(mean_squared_error(y_pred, y_true))\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "for i in range(0, config.layer_size):\n",
    "    model.add(tf.keras.layers.LSTM(\n",
    "        units=config.unit_size,\n",
    "        return_sequences=True,\n",
    "        batch_input_shape=(config.max_batch_size, len(config.feature), config.timestep),\n",
    "        go_backwards=True,\n",
    "        weights=[\n",
    "            np.array([\n",
    "                [0.5774, 0.5774, 0.5774, 0.5774],\n",
    "                [0.5774, 0.5774, 0.5774, 0.5774]\n",
    "            ]),\n",
    "            np.array([\n",
    "                [0.5774, 0.5774, 0.5774, 0.5774]\n",
    "            ]),\n",
    "            np.zeros([4])\n",
    "        ]\n",
    "    ))\n",
    "else:\n",
    "    # model.add(tf.keras.layers.Dense(units=1, activation='linear'))\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.SGD(learning_rate=config.learning_rate),\n",
    "        loss=mean_squared_error,\n",
    "        run_eagerly=True\n",
    "    )\n",
    "    model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.0797\n",
      "Epoch 2/50\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.0722\n",
      "Epoch 3/50\n",
      "52/52 [==============================] - 1s 28ms/step - loss: 0.0698\n",
      "Epoch 4/50\n",
      "52/52 [==============================] - 2s 32ms/step - loss: 0.0687\n",
      "Epoch 5/50\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.0680\n",
      "Epoch 6/50\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.0676\n",
      "Epoch 7/50\n",
      "52/52 [==============================] - 1s 29ms/step - loss: 0.0672\n",
      "Epoch 8/50\n",
      "52/52 [==============================] - 2s 39ms/step - loss: 0.0669\n",
      "Epoch 9/50\n",
      "52/52 [==============================] - 2s 31ms/step - loss: 0.0666\n",
      "Epoch 10/50\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 0.0664\n",
      "Epoch 11/50\n",
      "52/52 [==============================] - 1s 25ms/step - loss: 0.0662\n",
      "Epoch 12/50\n",
      "52/52 [==============================] - 1s 26ms/step - loss: 0.0660\n",
      "Epoch 13/50\n",
      "52/52 [==============================] - 1s 26ms/step - loss: 0.0658\n",
      "Epoch 14/50\n",
      "52/52 [==============================] - 1s 26ms/step - loss: 0.0656\n",
      "Epoch 15/50\n",
      "52/52 [==============================] - 1s 26ms/step - loss: 0.0655\n",
      "Epoch 16/50\n",
      "52/52 [==============================] - 1s 26ms/step - loss: 0.0653\n",
      "Epoch 17/50\n",
      "52/52 [==============================] - 1s 25ms/step - loss: 0.0652\n",
      "Epoch 18/50\n",
      "52/52 [==============================] - 1s 25ms/step - loss: 0.0651\n",
      "Epoch 19/50\n",
      "52/52 [==============================] - 1s 28ms/step - loss: 0.0650\n",
      "Epoch 20/50\n",
      "52/52 [==============================] - 1s 27ms/step - loss: 0.0649\n",
      "Epoch 21/50\n",
      "52/52 [==============================] - 2s 31ms/step - loss: 0.0648\n",
      "Epoch 22/50\n",
      "52/52 [==============================] - 1s 27ms/step - loss: 0.0647\n",
      "Epoch 23/50\n",
      "52/52 [==============================] - 2s 30ms/step - loss: 0.0646\n",
      "Epoch 24/50\n",
      "52/52 [==============================] - 2s 30ms/step - loss: 0.0645\n",
      "Epoch 25/50\n",
      "52/52 [==============================] - 2s 29ms/step - loss: 0.0645\n",
      "Epoch 26/50\n",
      "52/52 [==============================] - 2s 46ms/step - loss: 0.0644\n",
      "Epoch 27/50\n",
      "52/52 [==============================] - 2s 40ms/step - loss: 0.0643\n",
      "Epoch 28/50\n",
      "52/52 [==============================] - 3s 63ms/step - loss: 0.0643\n",
      "Epoch 29/50\n",
      "52/52 [==============================] - 3s 60ms/step - loss: 0.0642\n",
      "Epoch 30/50\n",
      "52/52 [==============================] - 3s 61ms/step - loss: 0.0641\n",
      "Epoch 31/50\n",
      "52/52 [==============================] - 1s 28ms/step - loss: 0.0641\n",
      "Epoch 32/50\n",
      "52/52 [==============================] - 2s 31ms/step - loss: 0.0640\n",
      "Epoch 33/50\n",
      "52/52 [==============================] - 1s 26ms/step - loss: 0.0640\n",
      "Epoch 34/50\n",
      "52/52 [==============================] - 1s 28ms/step - loss: 0.0639\n",
      "Epoch 35/50\n",
      "52/52 [==============================] - 1s 28ms/step - loss: 0.0639\n",
      "Epoch 36/50\n",
      "52/52 [==============================] - 2s 29ms/step - loss: 0.0638\n",
      "Epoch 37/50\n",
      "52/52 [==============================] - 1s 27ms/step - loss: 0.0638\n",
      "Epoch 38/50\n",
      "52/52 [==============================] - 2s 30ms/step - loss: 0.0638\n",
      "Epoch 39/50\n",
      "52/52 [==============================] - 2s 31ms/step - loss: 0.0637\n",
      "Epoch 40/50\n",
      "52/52 [==============================] - 2s 29ms/step - loss: 0.0637\n",
      "Epoch 41/50\n",
      "52/52 [==============================] - 2s 31ms/step - loss: 0.0637\n",
      "Epoch 42/50\n",
      "52/52 [==============================] - 1s 26ms/step - loss: 0.0636\n",
      "Epoch 43/50\n",
      "52/52 [==============================] - 1s 27ms/step - loss: 0.0636\n",
      "Epoch 44/50\n",
      "52/52 [==============================] - 1s 28ms/step - loss: 0.0636\n",
      "Epoch 45/50\n",
      "52/52 [==============================] - 2s 31ms/step - loss: 0.0635\n",
      "Epoch 46/50\n",
      "52/52 [==============================] - 2s 29ms/step - loss: 0.0635\n",
      "Epoch 47/50\n",
      "52/52 [==============================] - 1s 25ms/step - loss: 0.0635\n",
      "Epoch 48/50\n",
      "52/52 [==============================] - 1s 25ms/step - loss: 0.0634\n",
      "Epoch 49/50\n",
      "52/52 [==============================] - 2s 31ms/step - loss: 0.0634\n",
      "Epoch 50/50\n",
      "52/52 [==============================] - 1s 26ms/step - loss: 0.0634\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "\n",
    "batch_loss = list()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    shuffle=False,\n",
    "    epochs=config.max_epoch,\n",
    "    verbose=1,\n",
    "    batch_size=config.max_batch_size,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.LambdaCallback(\n",
    "            on_epoch_begin=None,\n",
    "            on_epoch_end=None,\n",
    "            on_batch_begin=None,\n",
    "            on_batch_end=lambda batch, logs=None: batch_loss.append(logs['loss']),\n",
    "            on_train_begin=None,\n",
    "            on_train_end=None,\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse = model.evaluate(\n",
    "#     X_test,\n",
    "#     y_test,\n",
    "#     verbose=1,\n",
    "#     batch_size=config.max_batch_size,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 29ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.2002306 ]],\n",
       "\n",
       "       [[0.18857674]],\n",
       "\n",
       "       [[0.18828028]],\n",
       "\n",
       "       [[0.18841618]]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test, verbose=1, batch_size=config.max_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "# testset_split = proyeksi_split(testset, timestep=config.timestep)\n",
    "# testset_split_reshape = np.reshape(testset_split, (testset_split.shape[0], len(config.feature), testset_split.shape[1]))\n",
    "# proyeksi = model.predict(testset_split_reshape, verbose=1, batch_size=config.max_batch_size)\n",
    "# for i in range(0, proyeksi.shape[0]):\n",
    "#     testset[i + config.timestep - 1] = np.reshape(proyeksi[i], (proyeksi[i].shape[1], len(config.feature)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 4ms/step\n",
      "7/7 [==============================] - 0s 4ms/step\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "9/9 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# for i in range(0, config.num_predict):\n",
    "#     testset_split = proyeksi_split(testset, timestep=config.timestep)\n",
    "#     testset_split_reshape = np.reshape(testset_split, (testset_split.shape[0], len(config.feature), testset_split.shape[1]))\n",
    "#     proyeksi = model.predict(testset_split_reshape, batch_size=config.max_batch_size)\n",
    "#     testset = np.concatenate((testset, np.reshape(proyeksi[-1], (proyeksi[-1].shape[1], len(config.feature)))), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testset = scaller.inverse_transform(testset)\n",
    "# testdateset = np.concatenate((testdateset, pd.to_datetime(pd.date_range(DATELIST[-1] + timedelta(days=1), periods=config.num_predict, freq='1d')).date), axis=0)\n",
    "\n",
    "# LABEL = np.concatenate((traindateset, testdateset), axis=0)\n",
    "\n",
    "# PREDICTIONS = pd.DataFrame(testset, columns=[config.feature]).set_index(pd.Series(testdateset))\n",
    "# PREDICTIONS.index = PREDICTIONS.index.to_series().apply(lambda x: datetime.strptime(x.strftime('%Y-%m-%d'), '%Y-%m-%d'))\n",
    "\n",
    "# HISTORY = pd.DataFrame(np.array(DATASETS[config.feature]), columns=[config.feature]).set_index(pd.Series(DATELIST))\n",
    "# HISTORY.index = HISTORY.index.to_series().apply(lambda x: datetime.strptime(x.strftime('%Y-%m-%d'), '%Y-%m-%d'))\n",
    "\n",
    "# START_DATE_FOR_PLOTTING = (DATELIST[-1] - timedelta(days=max((config.num_predict) * 3, 90))).strftime(\"%Y-%m-%d\")\n",
    "# blank_index = (DATELIST[-(DATASETS.shape[0] - train_size + config.timestep - 1)]).strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pylab import rcParams\n",
    "\n",
    "# rcParams['figure.figsize'] = 26, 10\n",
    "\n",
    "# plt.plot(PREDICTIONS.loc[START_DATE_FOR_PLOTTING:].index, PREDICTIONS.loc[START_DATE_FOR_PLOTTING:][config.prediction], color='orange', label='Proyeksi')\n",
    "# plt.plot(HISTORY.loc[START_DATE_FOR_PLOTTING:].index, HISTORY.loc[START_DATE_FOR_PLOTTING:][config.feature], label=config.feature)\n",
    "\n",
    "# plt.axvline(x = min(PREDICTIONS.index), color='green', linewidth=2, linestyle='--')\n",
    "# plt.grid(which='major', color='#cccccc', alpha=0.5)\n",
    "\n",
    "# plt.title('Prediksi dan Histori Curah Hujan', family='Arial', fontsize=12)\n",
    "# plt.xlabel('Timeline', family='Arial', fontsize=10)\n",
    "# plt.ylabel('Tingkat Nilai', family='Arial', fontsize=10)\n",
    "# plt.xticks(rotation=45, fontsize=8)\n",
    "# plt.legend(shadow=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2ec44ab71e55dcbd77e8108daeda65d530bc364476bc79d734b0ae94bd7b36be"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
